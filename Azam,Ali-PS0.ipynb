{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e3881e",
   "metadata": {},
   "source": [
    "### COSC 89.34: AI Agents (PS #0)\n",
    "Ali Azam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c4fb8",
   "metadata": {},
   "source": [
    "#### Part I\n",
    "1. (b) All three\n",
    "2. (b) Bias decreases, variance increases\n",
    "3. (b) A is positive semi-definite\n",
    "4. (a) Only on current state _s_ and action _a_\n",
    "5. (c) _f_ is strongly convex and has Lipschitz-continuous gradients\n",
    "6. (a) Distribution shift\n",
    "7. (c) $O(n^2)$\n",
    "8. (a) Network's final linear can achieve zero empirical error\n",
    "9. (b) Requires summing over all possible hypotheses\n",
    "10. (a) Noise inherent in data-generation process\n",
    "11. (b) SIMD\n",
    "12. (c) Design rules s.t. self-interested agents produce optinal outcome for designer\n",
    "13. (d) Pure strategy Nash Equilibria\n",
    "14. (c) If the new feature had no effect, there wouold be a 3% chance of observing an increase at least this large by chance\n",
    "15. (b) Bet (since pass = 5 vs. bet = $0.2 \\cdot 100 + 0.8 \\cdot -10 = 12$)\n",
    "16. (c) $\\frac{1}{1-0.9}=10.0$\n",
    "17. (b) UDP\n",
    "18. (a) Maximize ELBO $L(\\lambda)$\n",
    "19. (b) $Q$, $K$, $V$ are linear projections of the residual streams\n",
    "20. (d) Inference FLOPs per token _decrease_ as a power law model parameter count increases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab24a86c",
   "metadata": {},
   "source": [
    "#### Part II\n",
    "1. By discounted return,\n",
    "$$\n",
    "V = \\sum_{k=0}^{\\infty} \\gamma^k r = r\\sum_{k=0}^{\\infty} \\gamma^k = r \\cdot \\frac{1}{1-\\gamma}\n",
    "\\newline\n",
    "= 5 \\cdot \\frac{1}{1-0.8}  \\text{ since $\\gamma$=0.8 and reward=5}\n",
    "\\newline\n",
    "= 5 \\cdot 5 = 25\n",
    "$$\n",
    "\n",
    "2. Since $V_k(s_A) = 10$, $V_k(s_B) = 20$, $\\gamma = 0.9$, we have the outcomes:\n",
    "$\\newline$\n",
    "$s_A$ (with probability = 0.5), reward 0: return $0 + 0.9 \\cdot 10 = 9$\n",
    "$\\newline$\n",
    "$s_B$ (with probability = 0.5), reward 10: return 10 + 0.9 \\cdot 20 = 28$\n",
    "$\\newline$\n",
    "Using the formula for expctation:\n",
    "$\\newline V_k+1(s_A) = 0.5 \\cdot 9 + 0.5 \\cdot 28 = 18.5$\n",
    "\n",
    "3. Given $z_i = \\sum_{j=1}^{n} \\alpha_{ij} v_j$, and $v_j = W_Vx_j$, then for token 1:\n",
    "$\\newline$\n",
    "$z_1 = \\alpha_{11}v_1 + \\alpha{12}v_2 = \\alpha_{11}W_Vx_1 + \\alpha_{12}W_Vx_2$\n",
    "$\\newline$\n",
    "Residual add:\n",
    "$\\newline$\n",
    "$x_1' = x_1 + z_1 = x_1 + \\alpha_{11}v_1 + \\alpha{12}v_2 = \\alpha_{11}W_Vx_1 + \\alpha_{12}W_Vx_2$\n",
    "\n",
    "4. Not entirely sure about this one, but I'd expect something to do with sending tensors to the GPU in `inputs.to(\"cuda\")` from the CPU, e.g. loading the data or preprocessing.\n",
    "\n",
    "5. If that is the case, use a Straight-Through Estimator as follows: for the forward pass use the round/quantize `round(x)`, but  in the packward treat quantization as identity ($\\frac{\\partial \\texttt{round(x)}}{\\partial x})$ which should approximately equal 1 when inside the range. This can also be clipped for different usages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40e1540",
   "metadata": {},
   "source": [
    "#### Part III"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776d7af9",
   "metadata": {},
   "source": [
    "1. (a) and (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8494b441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.7.1 in /opt/homebrew/lib/python3.10/site-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision==0.22.1 in /opt/homebrew/lib/python3.10/site-packages (0.22.1)\n",
      "Requirement already satisfied: scikit-learn==1.7.2 in /opt/homebrew/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy==2.1.3 in /opt/homebrew/lib/python3.10/site-packages (2.1.3)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from torch==2.7.1) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/homebrew/lib/python3.10/site-packages (from torch==2.7.1) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/lib/python3.10/site-packages (from torch==2.7.1) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.10/site-packages (from torch==2.7.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.10/site-packages (from torch==2.7.1) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/lib/python3.10/site-packages (from torch==2.7.1) (2025.7.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/homebrew/lib/python3.10/site-packages (from torchvision==0.22.1) (11.3.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/homebrew/lib/python3.10/site-packages (from scikit-learn==1.7.2) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/lib/python3.10/site-packages (from scikit-learn==1.7.2) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/lib/python3.10/site-packages (from scikit-learn==1.7.2) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/lib/python3.10/site-packages (from sympy>=1.13.3->torch==2.7.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.10/site-packages (from jinja2->torch==2.7.1) (3.0.2)\n",
      "Test Accuracy: 94.88%\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch==2.7.1 torchvision==0.22.1 scikit-learn==1.7.2 numpy==2.1.3\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "SEED = 8934\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "\n",
    "class RCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2, 2) # 28x28 -> 14x14\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2  = nn.MaxPool2d(2, 2) # 14x14 -> 7x7\n",
    "        \n",
    "        self.fc = nn.Linear(32 * 7 * 7, 128)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def extract_features(loader):\n",
    "    features, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            f = model(x)\n",
    "            features.append(f.cpu().numpy())\n",
    "            labels.append(y.numpy())\n",
    "    return np.vstack(features), np.concatenate(labels)\n",
    "\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "train = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=256, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=256, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = RCNN().to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "train_features, train_labels = extract_features(train_loader)\n",
    "test_features, test_labels = extract_features(test_loader)\n",
    "\n",
    "svm = LinearSVC(C=1.0, random_state=SEED, max_iter=5000)\n",
    "svm.fit(train_features, train_labels)\n",
    "\n",
    "y_pred = svm.predict(test_features)\n",
    "acc = accuracy_score(test_labels, y_pred)\n",
    "\n",
    "print(f'Test Accuracy: {acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e62b16",
   "metadata": {},
   "source": [
    "(c) \n",
    "\n",
    "(i) A randomly initialized, untrained CNN is able to produce highly informative features fro the MNIST dataset, and the linear classifier on top of that achieves high accuracy without learning any convolutional weights. This is unusually high, but shows that that the CNN's dimensional features had well-developed strutures for identification between different classes.\n",
    "\n",
    "(ii) The CNN architecture is very helpful since the convolutions give a good sense of locality and object detection within the large dataset with small shifts. ReLU introduces nonlinearity into the system (and is more nontrivial than other features of the architecture), and pooling across the layers gives a good sense of invariant features for the same digit classes, for example, across the large dataset. Stacking all of these gives random nonlinear projections that perform very well in separating the MNIST classes. The success of the model relies more so on the architectural properties than on learned weights.\n",
    "\n",
    "(iii) Since the kernel already performs so well without supervised training, the CNN is already aligned with MNIST at initialization. The action of neural networks (such as CNN's convolutional kernels on the images initialized randomly) then training a linear classifier implicitly makes a kernel classification for it. This is supported by Rahimi and Recht, who show that nonlinear mappings by these random features followed by linear models can approximate kernel machines, since these features allow the data to have a high degree of linear separability (e.g. allowing us to train an SVC on the MNIST dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2488c7",
   "metadata": {},
   "source": [
    "2. Note: output exported from this code run on [Colab notebook](https://colab.research.google.com/drive/1Wz4j5giLM3tLikTdloOub2hUq22S3NGe?usp=sharing) with T4 GPU\n",
    "\n",
    "(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f437a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==2.7.1 matplotlib==3.10.3 numpy==2.1.3\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O shakespeare.txt\n",
    "\n",
    "DATA_PATH = './shakespeare.txt'\n",
    "\n",
    "with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    " \n",
    "SEED = 8934\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "\n",
    "BLOCK_SIZE = 128\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "d_model = 384\n",
    "n_heads = 6\n",
    "n_layers = 6\n",
    "d_ff = 4 * d_model\n",
    "dropout = 0.2\n",
    "lr = 3e-4\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "n_vocab = len(chars)\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [char_to_idx[c] for c in s]          # encode string to list of integers\n",
    "decode = lambda l: ''.join([idx_to_char[i] for i in l]) # decode list of integers to string\n",
    "\n",
    "n = int(0.9 * len(text))\n",
    "# Convert to tensors\n",
    "train_data = torch.tensor(encode(text[:n]), dtype=torch.long)\n",
    "val_data = torch.tensor(encode(text[n:]), dtype=torch.long)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def get_batch(split: str, device: str):\n",
    "    \"\"\"Helper method to generate small batch of inputs x and targets y.\"\"\"\n",
    "    \n",
    "    d = train_data if split == \"train\" else val_data\n",
    "   \n",
    "    # random starting positions\n",
    "    ix = torch.randint(0, len(d) - BLOCK_SIZE - 1, (BATCH_SIZE,))\n",
    "    x = torch.stack([d[i : i + BLOCK_SIZE] for i in ix])\n",
    "    y = torch.stack([d[i + 1 : i + BLOCK_SIZE + 1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model: nn.Module, device: str):\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = []\n",
    "        for _ in range(eval_iters):\n",
    "            x, y = get_batch(split, device)\n",
    "            _, loss = model(x, y)\n",
    "            losses.append(loss.item())\n",
    "        out[split] = float(np.mean(losses))\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f05a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.resid_drop = nn.Dropout(dropout)\n",
    "\n",
    "        # Causal mask, [1, 1, T, T] broadcastable\n",
    "        mask = torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE, dtype=torch.bool))\n",
    "        self.register_buffer(\"causal_mask\", mask.view(1, 1, BLOCK_SIZE, BLOCK_SIZE), persistent=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: [B, T, C]\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        q = self.q_proj(x)  # [B, T, C]\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        # split heads: [B, nh, T, hd]\n",
    "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # scaled dot-product attention\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)  # [B, nh, T, T]\n",
    "\n",
    "        # apply causal mask for current T\n",
    "        mask = self.causal_mask[:, :, :T, :T]\n",
    "        att = att.masked_fill(~mask, float(\"-inf\"))\n",
    "\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "\n",
    "        y = att @ v  # [B, nh, T, hd]\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # [B, T, C]\n",
    "\n",
    "        y = self.out_proj(y)\n",
    "        y = self.resid_drop(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    # Pre-LN block: x + Attn(LN(x)), x + FFN(LN(x))\n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = BLOCK_SIZE  # Store block_size as instance variable\n",
    "\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(BLOCK_SIZE, d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "        # weight tying is optional; keep simple and explicit\n",
    "        # self.lm_head.weight = self.tok_emb.weight\n",
    "\n",
    "    def forward(self, idx: torch.Tensor, targets: torch.Tensor | None = None):\n",
    "        # idx: [B, T]\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.block_size\n",
    "\n",
    "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)  # [1, T]\n",
    "        x = self.tok_emb(idx) + self.pos_emb(pos)  # [B, T, C]\n",
    "        x = self.drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)  # [B, T, vocab]\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, self.vocab_size), targets.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx: torch.Tensor, max_new_tokens: int, temperature: float = 1.0, top_k: int | None = None):\n",
    "        self.eval()\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]  # crop\n",
    "            logits, _ = self(idx_cond, targets=None)\n",
    "            logits = logits[:, -1, :]  # last step, [B, vocab]\n",
    "\n",
    "            logits = logits / max(temperature, 1e-6)\n",
    "\n",
    "            if top_k is not None and top_k > 0:\n",
    "                v, _ = torch.topk(logits, k=min(top_k, logits.size(-1)))\n",
    "                kth = v[:, -1].unsqueeze(-1)\n",
    "                logits = torch.where(logits < kth, torch.full_like(logits, float(\"-inf\")), logits)\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)  # [B, 1]\n",
    "            idx = torch.cat([idx, next_id], dim=1)\n",
    "        return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c990ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecoderOnlyTransformer(n_vocab).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "for i in range(1, max_iters + 1):\n",
    "    xb, yb = get_batch(\"train\", device)\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % eval_interval == 0 or i == 1:\n",
    "        losses = estimate_loss(model, device)\n",
    "        print(f\"{i} | train loss: {losses['train']:.4f} | val loss: {losses['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1859cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_max_new_tokens = 500\n",
    "\n",
    "prompt = \"JULIET:\"\n",
    "x0 = torch.tensor(encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\n",
    "out = model.generate(\n",
    "    x0,\n",
    "    max_new_tokens=500,\n",
    "    temperature=0.85,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "juliet = decode(out[0].tolist())\n",
    "print(\"\\n--- SAMPLE ---\\n\")\n",
    "print(juliet)\n",
    "print(\"\\n--- END SAMPLE ---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1b6603",
   "metadata": {},
   "source": [
    "**Output:**\n",
    "```\n",
    "1 | train loss: 3.7021 | val loss: 3.7315\n",
    "500 | train loss: 1.8726 | val loss: 1.9846\n",
    "1000 | train loss: 1.5964 | val loss: 1.7753\n",
    "1500 | train loss: 1.4704 | val loss: 1.6658\n",
    "2000 | train loss: 1.3979 | val loss: 1.6125\n",
    "2500 | train loss: 1.3475 | val loss: 1.5668\n",
    "3000 | train loss: 1.3119 | val loss: 1.5441\n",
    "3500 | train loss: 1.2763 | val loss: 1.5168\n",
    "4000 | train loss: 1.2531 | val loss: 1.5139\n",
    "4500 | train loss: 1.2235 | val loss: 1.5008\n",
    "5000 | train loss: 1.2027 | val loss: 1.4926\n",
    "\n",
    "--- SAMPLE ---\n",
    "\n",
    "JULIET:\n",
    "God knows he hath not the Capitol' set:\n",
    "And why, sir? then, pity will I have.\n",
    "\n",
    "CLARENCE:\n",
    "But these Earl of Norfolk, The senators we was\n",
    "so deadly tender heaven the cause throats him approach:\n",
    "His world, of my dispatch,\n",
    "So far ore grace as the potion doth the royal duke,\n",
    "Whose seven branch and give of life. When she was speak, but her,\n",
    "And thou hast let a party but complime monarches\n",
    "That they shall for my sake a care for ever\n",
    "Of her action can pash the devil of wild:\n",
    "The king of Went, that quee\n",
    "\n",
    "--- END SAMPLE ---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c768c9a0",
   "metadata": {},
   "source": [
    "3. Note: output exported from this code run on [Colab notebook](https://colab.research.google.com/drive/1Wz4j5giLM3tLikTdloOub2hUq22S3NGe?usp=sharing) with T4 GPU \n",
    "\n",
    "(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de35ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O shakespeare.txt\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "DATA_PATH = './shakespeare.txt'\n",
    "N_MERGES = 1000\n",
    "\n",
    "with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583bef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pairs(tokens):\n",
    "    pairs = Counter()\n",
    "    if len(tokens) < 2:\n",
    "        return pairs\n",
    "    for i in range(len(tokens) - 1):\n",
    "        pairs[(tokens[i], tokens[i + 1])] += 1\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def merge_pair(tokens, pair, merged):\n",
    "    a, b = pair\n",
    "    out = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i < len(tokens) - 1 and tokens[i] == a and tokens[i + 1] == b:\n",
    "            out.append(merged)\n",
    "            i += 2\n",
    "        else:\n",
    "            out.append(tokens[i])\n",
    "            i += 1\n",
    "    return out\n",
    "\n",
    "\n",
    "def train_bpe(text, num_merges):\n",
    "    tokens = list(text)\n",
    "    vocab = set(tokens)\n",
    "    merges = []\n",
    "    merge_ranks = {}\n",
    "    \n",
    "    for _ in range(num_merges):\n",
    "        pairs = count_pairs(tokens)\n",
    "        if not pairs:\n",
    "            break\n",
    "        \n",
    "        best_pair, count = pairs.most_common(1)[0]\n",
    "        if count < 2:\n",
    "            break\n",
    "        \n",
    "        merged = best_pair[0] + best_pair[1]\n",
    "        tokens = merge_pair(tokens, best_pair, merged)\n",
    "        \n",
    "        merges.append((best_pair, merged))\n",
    "        merge_ranks[best_pair] = len(merges) - 1\n",
    "        vocab.add(merged)\n",
    "    \n",
    "    return merges, merge_ranks, vocab\n",
    "\n",
    "\n",
    "def encode(s, merge_ranks):\n",
    "    \"\"\"Encode string using learned merge rules.\"\"\"\n",
    "    tokens = list(s)\n",
    "    \n",
    "    if len(tokens) < 2 or not merge_ranks:\n",
    "        return tokens\n",
    "    \n",
    "    while True:\n",
    "        pairs = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]\n",
    "        candidates = [(merge_ranks[p], p) for p in pairs if p in merge_ranks]\n",
    "        \n",
    "        if not candidates:\n",
    "            break\n",
    "        \n",
    "        _, best_pair = min(candidates)\n",
    "        merged = best_pair[0] + best_pair[1]\n",
    "        tokens = merge_pair(tokens, best_pair, merged)\n",
    "        \n",
    "        if len(tokens) < 2:\n",
    "            break\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa7f997",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges, merge_ranks, vocab = train_bpe(text, N_MERGES)\n",
    "\n",
    "print(f\"Learned merges: {len(merges)}\")\n",
    "\n",
    "# Show merge rules\n",
    "show = merges[-50:]\n",
    "print(f\"\\n--- MERGE RULES (LAST 50) ---\")\n",
    "\n",
    "# (a)\n",
    "start = len(merges) - len(show)\n",
    "for i, (pair, merged) in enumerate(show, start=start):\n",
    "    print(f\"{i:04d}: {pair[0]!r} + {pair[1]!r} -> {merged!r}\")\n",
    "\n",
    "# (b)\n",
    "horatio = \"Alas, poor Yorick! I knew him, Horatio:\"\n",
    "tokens = encode(horatio, merge_ranks)\n",
    "\n",
    "print(\"\\n--- ENCODING EXAMPLE ---\")\n",
    "print(\"Input:\", repr(horatio))\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token count:\", len(tokens))\n",
    "print(\"Reconstructed:\", repr(\"\".join(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9465a1bd",
   "metadata": {},
   "source": [
    "**Output:**\n",
    "\n",
    "```\n",
    "Learned merges: 1000\n",
    "\n",
    "--- MERGE RULES (LAST 50) ---\n",
    "0950: ' d' + 'o ' -> ' do '\n",
    "0951: 'th' + 'ese ' -> 'these '\n",
    "0952: 'ee' + 'ch' -> 'eech'\n",
    "0953: 'H' + 'en' -> 'Hen'\n",
    "0954: ' m' + 'e, ' -> ' me, '\n",
    "0955: 'e' + ':\\n' -> 'e:\\n'\n",
    "0956: '.\\n\\nP' + 'ETRUCHIO:\\n' -> '.\\n\\nPETRUCHIO:\\n'\n",
    "0957: ' to ' + 'the ' -> ' to the '\n",
    "0958: 'you' + ' m' -> 'you m'\n",
    "0959: 'S' + 't' -> 'St'\n",
    "0960: 'a' + 'il' -> 'ail'\n",
    "0961: 'c' + 'rown' -> 'crown'\n",
    "0962: 'ou' + 't' -> 'out'\n",
    "0963: ' a' + 'll ' -> ' all '\n",
    "0964: ' w' + 'as ' -> ' was '\n",
    "0965: 'QUEEN ' + 'EL' -> 'QUEEN EL'\n",
    "0966: 'QUEEN EL' + 'I' -> 'QUEEN ELI'\n",
    "0967: 'QUEEN ELI' + 'Z' -> 'QUEEN ELIZ'\n",
    "0968: 'QUEEN ELIZ' + 'AB' -> 'QUEEN ELIZAB'\n",
    "0969: 'QUEEN ELIZAB' + 'ET' -> 'QUEEN ELIZABET'\n",
    "0970: 'QUEEN ELIZABET' + 'H' -> 'QUEEN ELIZABETH'\n",
    "0971: 'n' + 'at' -> 'nat'\n",
    "0972: 't' + 'ongu' -> 'tongu'\n",
    "0973: 'w' + 'ard' -> 'ward'\n",
    "0974: 'e' + ';\\n' -> 'e;\\n'\n",
    "0975: 'or' + 'row' -> 'orrow'\n",
    "0976: 'in' + 't' -> 'int'\n",
    "0977: 'thou' + 'gh' -> 'though'\n",
    "0978: 'e' + 'e ' -> 'ee '\n",
    "0979: '?\\n\\n' + 'B' -> '?\\n\\nB'\n",
    "0980: 're' + 'e ' -> 'ree '\n",
    "0981: 'us' + 'b' -> 'usb'\n",
    "0982: ';' + ' but ' -> '; but '\n",
    "0983: 'a' + ' w' -> 'a w'\n",
    "0984: 'w' + 'ay ' -> 'way '\n",
    "0985: '.\\n\\nM' + 'ENENIUS:\\n' -> '.\\n\\nMENENIUS:\\n'\n",
    "0986: 'c' + 'ar' -> 'car'\n",
    "0987: 'ell' + 'ow' -> 'ellow'\n",
    "0988: 'by ' + 'the ' -> 'by the '\n",
    "0989: 'p' + 'a' -> 'pa'\n",
    "0990: '?\\n\\n' + 'P' -> '?\\n\\nP'\n",
    "0991: 'la' + 'ck' -> 'lack'\n",
    "0992: 's' + 'ay' -> 'say'\n",
    "0993: 'g' + 'reat ' -> 'great '\n",
    "0994: ' g' + 'ood ' -> ' good '\n",
    "0995: 'is ' + 'the ' -> 'is the '\n",
    "0996: 'S' + ' ' -> 'S '\n",
    "0997: 'to ' + 'my ' -> 'to my '\n",
    "0998: 'S' + 'erv' -> 'Serv'\n",
    "0999: 'hea' + 'ven' -> 'heaven'\n",
    "\n",
    "--- ENCODING EXAMPLE ---\n",
    "Input: 'Alas, poor Yorick! I knew him, Horatio:'\n",
    "Tokens: ['A', 'la', 's, ', 'po', 'or ', 'Y', 'or', 'ick', '! ', 'I ', 'k', 'new', ' him', ', ', 'H', 'or', 'at', 'io', ':']\n",
    "Token count: 19\n",
    "Reconstructed: 'Alas, poor Yorick! I knew him, Horatio:'\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
